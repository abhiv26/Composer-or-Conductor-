{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0d97b13-c7c7-43fa-9c9d-4b6cfa058888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import anthropic\n",
    "import soundfile as sf\n",
    "import csv\n",
    "from mutagen.mp3 import MP3\n",
    "from mutagen.id3 import ID3\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1b8614-5048-4004-9bc3-d254a9d5f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_filename(filename):\n",
    "    filename = re.sub(r'[<>:\"/\\\\|?*]', '_', filename) \n",
    "    filename = re.sub(r'\\s+', '_', filename)  \n",
    "    return filename.strip('_')\n",
    "\n",
    "def extract_piece_number(file_name):\n",
    "    match = re.search(r'\\b(Symphony|Concerto|Sonata|Op|No\\.?|No|Piece)\\s*\\.?\\s*(\\d+)', file_name, re.IGNORECASE)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}_{match.group(2)}\"  # E.g., Symphony_1, Concerto_2\n",
    "    return \"\"\n",
    "\n",
    "def split_audio(file_path, output_folder, segment_duration=29):\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(file_path)\n",
    "    total_duration = librosa.get_duration(y=y, sr=sr)\n",
    "    \n",
    "    # Calculate the number of segments\n",
    "    segment_count = int(total_duration // segment_duration)\n",
    "    \n",
    "    # Extract the piece type and number from the file name\n",
    "    file_name = os.path.basename(file_path)\n",
    "    piece_number = extract_piece_number(file_name)\n",
    "    \n",
    "    # Ensure the output folder exists\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Split the audio and save each segment\n",
    "    for i in range(segment_count):\n",
    "        start_time = i * segment_duration\n",
    "        end_time = start_time + segment_duration\n",
    "        segment = y[int(start_time * sr):int(end_time * sr)]\n",
    "        \n",
    "        # Format the file name based on the piece number and segment number\n",
    "        if piece_number:\n",
    "            segment_file_name = f\"{piece_number}_Segment_{i+1}.wav\"\n",
    "        else:\n",
    "            # Fallback if no piece number is found\n",
    "            segment_file_name = f\"Piece_Segment_{i+1}.wav\"\n",
    "        \n",
    "        segment_file_name = sanitize_filename(segment_file_name)\n",
    "        segment_file_path = os.path.join(output_folder, segment_file_name)\n",
    "        \n",
    "        # Save the segment\n",
    "        sf.write(segment_file_path, segment, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7746173-8ad2-40c3-9b02-a5a1ce559c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from audio snippet function\n",
    "def extract_features(file_path):\n",
    "    y, sr = librosa.load(file_path, duration=30)  \n",
    "    \n",
    "    # Extract features\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "    key = librosa.core.estimate_tuning(y=y, sr=sr)\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    rhythm_patterns = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr)\n",
    "    pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr)\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "    harmonic_progression = librosa.feature.chroma_cqt(y=y_harmonic, sr=sr)\n",
    "    # Print shapes for verification\n",
    "\n",
    "    # Compute mean values of the features\n",
    "    mfccs_mean = np.mean(mfccs, axis=1)\n",
    "    chroma_mean = np.mean(chroma, axis=1)\n",
    "    spectral_contrast_mean = np.mean(spectral_contrast, axis=1)\n",
    "    #tempo_array = np.array([tempo])\n",
    "    key_array = np.array([key])\n",
    "    rhythm_mean = np.mean(rhythm_patterns, axis=1)\n",
    "    pitch_range = np.array([pitches[pitches > 0].min(), pitches[pitches > 0].max()])\n",
    "    rms_mean = np.mean(rms)\n",
    "    harmonic_mean = np.mean(harmonic_progression, axis=1)\n",
    "    # Combine features into a single 1D array\n",
    "    features = np.concatenate([\n",
    "        mfccs_mean,\n",
    "        chroma_mean,\n",
    "        spectral_contrast_mean,\n",
    "        tempo,\n",
    "        key_array,\n",
    "        rhythm_mean,\n",
    "        pitch_range,\n",
    "        [rms_mean],\n",
    "        harmonic_mean\n",
    "    ])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86033390-2e5b-495f-8fff-e334ca0bc5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_prompt(features):\n",
    "    # Decompose the features into their respective components\n",
    "    mfccs_mean = features[:13]\n",
    "    chroma_mean = features[13:25]\n",
    "    spectral_contrast_mean = features[25:32]\n",
    "    tempo = features[32]\n",
    "    key = features[33]\n",
    "    rhythm_mean = features[34:44]\n",
    "    pitch_range = features[44:46]\n",
    "    rms_mean = features[46]\n",
    "    harmonic_mean = features[47:59]\n",
    "\n",
    "    # Create a descriptive text format\n",
    "    prompt = (\n",
    "        \"Given the following audio features extracted from a music snippet:\\n\"\n",
    "        f\"- MFCCs Mean (13 coefficients): {mfccs_mean.tolist()}\\n\"\n",
    "        f\"- Chroma Mean (12 coefficients): {chroma_mean.tolist()}\\n\"\n",
    "        f\"- Spectral Contrast Mean (7 coefficients): {spectral_contrast_mean.tolist()}\\n\"\n",
    "        f\"- Tempo (bpm): {tempo}\\n\"\n",
    "        f\"- Estimated Key/Tonality: {key}\\n\"\n",
    "        f\"- Rhythm Patterns Mean (10 coefficients): {rhythm_mean.tolist()}\\n\"\n",
    "        f\"- Pitch Range (min, max): {pitch_range.tolist()}\\n\"\n",
    "        f\"- Dynamics (RMS Mean): {rms_mean}\\n\"\n",
    "        f\"- Harmonic Progression Mean (12 coefficients): {harmonic_mean.tolist()}\\n\\n\"\n",
    "        \"Predict the emotions that are most likely to be felt by the listener in one to two words at most and no other text.\"\n",
    "    )\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c8fb25e-ac04-4a7e-a71b-d89db1e0cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = anthropic.Anthropic(api_key=\"insert your key\") # $5.00 worth of promotional credit from Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8899f9af-aae3-4ceb-99f7-5e196b4fa5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(prompt):\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-sonnet-20240229\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff403be3-607a-427d-9e5e-0eb6735067a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(filename, composer, piece_name, predicted_emotions):\n",
    "    \n",
    "    file_exists = os.path.isfile(filename)\n",
    "    \n",
    "    with open(filename, 'a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"Composer\", \"Piece Name\", \"Predicted Emotions\"])\n",
    "        writer.writerow([composer, piece_name, predicted_emotions])\n",
    "\n",
    "def process_audio_segments(folder_path, composer, output_file):\n",
    "    audio_files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".wav\")],\n",
    "                         key=lambda x: int(x.split('_')[-1].replace('.wav', '')))    \n",
    "    for filename in audio_files:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        piece_name = filename.replace(\".wav\", \"\")\n",
    "        new_features = extract_features(file_path)\n",
    "        prompt = create_llm_prompt(new_features)\n",
    "        predicted_emotions = get_llm_response(prompt)\n",
    "        save_to_csv(output_file, composer, piece_name, predicted_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "934edcae-65ab-4f18-a229-302d0e257398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_piece_name_from_mp3(mp3_file_path):\n",
    "    try:\n",
    "        audio = MP3(mp3_file_path, ID3=ID3)\n",
    "        title = audio.tags.get('TIT2')  # Get the title (TIT2 is the ID3 frame for the title)\n",
    "        if title:\n",
    "            return title.text[0]\n",
    "        else:\n",
    "            return os.path.splitext(os.path.basename(mp3_file_path))[0]  # Fallback to filename without extension\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting title from {mp3_file_path}: {e}\")\n",
    "        return os.path.splitext(os.path.basename(mp3_file_path))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0df258e-e631-4ce4-8b1b-303895eb52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def automate_audio_processing(input_folder, output_folder, output_file, composer,segment_duration=29):\n",
    "    mp3_files = [f for f in os.listdir(input_folder) if f.endswith(\".mp3\")]\n",
    "    \n",
    "    for mp3_file in mp3_files:\n",
    "        mp3_file_path = os.path.join(input_folder, mp3_file)\n",
    "        piece_name = get_piece_name_from_mp3(mp3_file_path)\n",
    "        \n",
    "        piece_output_folder = os.path.join(output_folder, piece_name)\n",
    "        split_audio(mp3_file_path, piece_output_folder, segment_duration)\n",
    "        process_audio_segments(piece_output_folder, composer, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78667e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = r\"specify path\"  # Folder containing audio files from audio conversion process\n",
    "output_folder = r\"specify path\"  # Folder to save the split audio segments\n",
    "output_file = r\"specify path\"  # Output CSV containing preds\n",
    "composer = \"Ludwig van Beethoven\"\n",
    "automate_audio_processing(input_folder, output_folder, output_file, composer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
